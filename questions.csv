Question,Option A,Option B,Option C,Option D,Correct Answer
What is the primary goal of prompting techniques in generative AI?,a) To make the models more efficient,b) To guide the models to generate desired outputs,c) To reduce the computational cost of training,d) To eliminate the need for fine-tuning,Option B
Which of the following is NOT a type of prompting technique discussed in the video?,a) Chain-of-Thought (CoT) prompting,b) Zero-shot prompting,c) Adversarial prompting,d) Self-consistency prompting,Option C
How does Chain-of-Thought (CoT) prompting work?,a) By providing task-specific training data,b) By encouraging the model to break down complex problems into intermediate steps,c) By generating multiple outputs and selecting the most consistent answer,d) By dynamically adjusting the prompt based on previous outputs,Option B
Zero-shot prompting enables models to perform tasks without:,a) Any pre-training,b) Task-specific training data,c) Access to external knowledge sources,d) Human intervention,Option B
Few-shot prompting involves:,a) Providing a large number of examples to guide the model's output,b) Generating multiple outputs for a given prompt,c) Providing a small number of examples to guide the model's output,d) Dynamically adjusting the prompt based on the task at hand,Option C
Self-consistency prompting improves the model's accuracy and robustness by:,a) Generating multiple outputs and selecting the most consistent answer,b) Breaking down complex problems into intermediate steps,c) Leveraging the model's pre-existing knowledge,d) Providing task-specific training data,Option A
Which type of prompting combines LLMs with external knowledge sources?,a) Multimodal prompting,b) Adaptive prompting,c) Retrieval-augmented prompting,d) Zero-shot prompting,Option C
Multimodal prompting involves using:,a) Only textual inputs,b) Only visual inputs,"c) Multiple input modalities, such as text, images, and audio",d) No inputs at all,Option C
Adaptive prompting allows for:,a) More efficient and targeted guidance,b) Generating multiple outputs,c) Accessing external knowledge sources,d) Performing tasks without task-specific training data,Option A
Which of the following is an example of a task that can benefit from multimodal prompting?,a) Language translation,b) Arithmetic reasoning,c) Image captioning,d) Fact-checking,Option C
Which research paper introduced the concept of Chain-of-Thought (CoT) prompting?,a) Brown et al. (2020),b) Wei et al. (2022),c) Wang et al. (2022),d) Nye et al. (2021),Option B
"GPT-3, introduced by Brown et al. (2020), demonstrated impressive capabilities in:",a) Zero-shot and few-shot learning,b) Multimodal prompting,c) Adaptive prompting,d) Self-consistency prompting,Option A
Which prompting technique leverages the model's ability to generate diverse responses and uses consistency among those responses as a measure of correctness?,a) Chain-of-Thought (CoT) prompting,b) Zero-shot prompting,c) Few-shot prompting,d) Self-consistency prompting,Option D
Retrieval-augmented prompting has been applied to tasks such as:,a) Open-domain question answering and fact-checking,b) Image captioning and visual question answering,c) Arithmetic reasoning and commonsense reasoning,d) Controllable text generation and story generation,Option A
"The DALLÂ·E model, showcased by Ramesh et al. (2021), demonstrates the power of:",a) Chain-of-Thought (CoT) prompting,b) Zero-shot prompting,c) Multimodal prompting,d) Adaptive prompting,Option C
Which prompting technique dynamically adjusts the prompt based on the model's previous outputs or the task at hand?,a) Chain-of-Thought (CoT) prompting,b) Zero-shot prompting,c) Few-shot prompting,d) Adaptive prompting,Option D
Keskar et al. (2019) explored the concept of meta-learning for:,a) Chain-of-Thought (CoT) prompting,b) Zero-shot prompting,c) Few-shot prompting,d) Adaptive prompting,Option D
Which type of LLM is typically used for generative tasks where the output length differs from the input length?,a) Decoder-only transformers,b) Encoder-decoder transformers,c) Both decoder-only and encoder-decoder transformers,d) Neither decoder-only nor encoder-decoder transformers,Option B
Minor modifications in prompts can lead to:,a) No change in the model's output,b) Slightly different outputs,c) Significantly different outputs,d) More efficient model training,Option C
Self-consistency prompting was proposed by:,a) Brown et al. (2020),b) Wei et al. (2022),c) Wang et al. (2022),d) Nye et al. (2021),Option C
